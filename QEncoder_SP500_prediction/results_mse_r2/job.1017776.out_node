==========================================
SLURM_CLUSTER_NAME = param-shivay
SLURM_JOB_ACCOUNT = comp_sci_engg
SLURM_JOB_ID = 1017776
SLURM_JOB_NAME = printjob
SLURM_JOB_NODELIST = cn001
SLURM_JOB_USER = soumyadeep.das.cse21.itbhu
SLURM_JOB_UID = 5621
SLURM_JOB_PARTITION = cpu
SLURM_TASK_PID = 71271
SLURM_SUBMIT_DIR = /home/soumyadeep.das.cse21.itbhu/quantum-ml/LexiQL
SLURM_CPUS_ON_NODE = 1
SLURM_NTASKS = 
SLURM_TASK_PID = 71271
==========================================
hello world
module loaded
env loaded
python is running
number of stocks: 506

 Training Encoder...

Encoder Loss: 0.031 Iteration: 20
Encoder Loss: 0.029 Iteration: 40
Encoder Loss: 0.027 Iteration: 60
Encoder Loss: 0.03 Iteration: 80
Encoder Loss: 0.03 Iteration: 100
Encoder Loss: 0.031 Iteration: 120
Encoder Loss: 0.034 Iteration: 140
Encoder Loss: 0.031 Iteration: 160
Encoder Loss: 0.029 Iteration: 180
Encoder Loss: 0.032 Iteration: 200
Encoder Loss: 0.031 Iteration: 220
Encoder Loss: 0.028 Iteration: 240
Encoder Loss: 0.029 Iteration: 260
Encoder Loss: 0.034 Iteration: 280
Encoder Loss: 0.032 Iteration: 300
Training Started... 
0: 0.18726536631584167
Training Epoch 0
train_metrics: 
R2: -6.920363903045654
MSE: 1226457.0
test metrics:
R2: -8.73529052734375
MSE: 1479340.25
before val features
MSE diff_p 0
----------
1: 0.10066357254981995
Training Epoch 1
train_metrics: 
R2: -57.09842300415039
MSE: 659276.0
test metrics:
R2: -90.41366577148438
MSE: 787898.0
MSE diff_p 1
----------
2: 0.06670237332582474
Training Epoch 2
train_metrics: 
R2: -10.550265312194824
MSE: 436853.875
test metrics:
R2: -19.295398712158203
MSE: 553625.5
MSE diff_p 2
----------
3: 0.10739754140377045
Training Epoch 3
train_metrics: 
R2: -43.200077056884766
MSE: 703378.8125
test metrics:
R2: -47.953269958496094
MSE: 961067.625
MSE diff_p 3
----------
4: 0.0749293863773346
Training Epoch 4
train_metrics: 
R2: -22.56621551513672
MSE: 490735.125
test metrics:
R2: -20.60150718688965
MSE: 461284.46875
MSE diff_p 4
----------
5: 0.0346287302672863
Training Epoch 5
train_metrics: 
R2: -3.2263903617858887
MSE: 226793.953125
test metrics:
R2: -3.8146138191223145
MSE: 358786.25
MSE diff_p 5
----------
6: 0.005830730311572552
Training Epoch 6
train_metrics: 
R2: 0.8840710520744324
MSE: 38187.203125
test metrics:
R2: 0.9040598273277283
MSE: 40532.62109375
MSE diff_p 6
----------
7: 0.00731025030836463
Training Epoch 7
train_metrics: 
R2: 0.8317223787307739
MSE: 47877.03125
test metrics:
R2: 0.9121615290641785
MSE: 37508.35546875
MSE diff_p 7
----------
8: 0.019172724336385727
Training Epoch 8
train_metrics: 
R2: 0.5568080544471741
MSE: 125567.9375
test metrics:
R2: 0.6408172845840454
MSE: 113718.9765625
MSE diff_p 8
----------
9: 0.016726577654480934
Training Epoch 9
train_metrics: 
R2: 0.6584256291389465
MSE: 109547.375
test metrics:
R2: 0.6839753985404968
MSE: 109784.171875
MSE diff_p 9
----------
10: 0.057726338505744934
Training Epoch 10
train_metrics: 
R2: -0.6197357177734375
MSE: 378067.15625
test metrics:
R2: -0.3774172067642212
MSE: 259257.40625
MSE diff_p 10
----------
11: 0.019155088812112808
Training Epoch 11
train_metrics: 
R2: 0.10574406385421753
MSE: 125452.4375
test metrics:
R2: -1.165647268295288
MSE: 260777.734375
MSE diff_p 11
----------
12: 0.017565295100212097
Training Epoch 12
train_metrics: 
R2: 0.692313551902771
MSE: 115040.40625
test metrics:
R2: 0.7581396102905273
MSE: 95386.46875
MSE diff_p 12
----------
13: 0.016558418050408363
Training Epoch 13
train_metrics: 
R2: 0.6566609144210815
MSE: 108446.046875
test metrics:
R2: 0.5372666716575623
MSE: 135037.40625
MSE diff_p 13
----------
14: 0.016863003373146057
Training Epoch 14
train_metrics: 
R2: 0.7249058485031128
MSE: 110440.890625
test metrics:
R2: 0.8244429230690002
MSE: 79602.5078125
MSE diff_p 14
----------
15: 0.005354192573577166
Training Epoch 15
train_metrics: 
R2: 0.8979949951171875
MSE: 35066.2109375
test metrics:
R2: 0.8886798620223999
MSE: 42597.67578125
MSE diff_p 15
----------
16: 0.004042533226311207
Training Epoch 16
train_metrics: 
R2: 0.9314295649528503
MSE: 26475.767578125
test metrics:
R2: 0.9305366277694702
MSE: 29247.603515625
MSE diff_p 16
----------
17: 0.0028472000267356634
Training Epoch 17
train_metrics: 
R2: 0.945305347442627
MSE: 18647.16796875
test metrics:
R2: 0.9319770336151123
MSE: 27828.447265625
MSE diff_p 17
----------
18: 0.0020327777601778507
Training Epoch 18
train_metrics: 
R2: 0.9601047039031982
MSE: 13313.2734375
test metrics:
R2: 0.9412391185760498
MSE: 23966.240234375
MSE diff_p 18
----------
19: 0.002257220447063446
Training Epoch 19
train_metrics: 
R2: 0.9619213938713074
MSE: 14783.2158203125
test metrics:
R2: 0.9300714731216431
MSE: 28148.0859375
MSE diff_p 19
----------
20: 0.001957252388820052
Training Epoch 20
train_metrics: 
R2: 0.9667136073112488
MSE: 12818.634765625
test metrics:
R2: 0.9508218169212341
MSE: 21200.3359375
MSE diff_p 20
----------
21: 0.001757770194672048
Training Epoch 21
train_metrics: 
R2: 0.971515417098999
MSE: 11512.1650390625
test metrics:
R2: 0.9356945157051086
MSE: 26517.623046875
MSE diff_p 21
----------
22: 0.0020929002203047276
Training Epoch 22
train_metrics: 
R2: 0.9582052230834961
MSE: 13707.0322265625
test metrics:
R2: 0.9515710473060608
MSE: 20863.595703125
MSE diff_p 22
----------
23: 0.0020040336530655622
Training Epoch 23
train_metrics: 
R2: 0.9653517603874207
MSE: 13125.017578125
test metrics:
R2: 0.9310400485992432
MSE: 28687.033203125
MSE diff_p 23
----------
24: 0.002265153219923377
Training Epoch 24
train_metrics: 
R2: 0.9591218829154968
MSE: 14835.1689453125
test metrics:
R2: 0.952541708946228
MSE: 20188.396484375
MSE diff_p 24
----------
25: 0.0024473373778164387
Training Epoch 25
train_metrics: 
R2: 0.9556519985198975
MSE: 16028.34765625
test metrics:
R2: 0.9220305681228638
MSE: 32189.29296875
MSE diff_p 25
----------
26: 0.0023040063679218292
Training Epoch 26
train_metrics: 
R2: 0.9597700834274292
MSE: 15089.6328125
test metrics:
R2: 0.9571532011032104
MSE: 19177.841796875
MSE diff_p 26
----------
27: 0.0021105725318193436
Training Epoch 27
train_metrics: 
R2: 0.962909460067749
MSE: 13822.7724609375
test metrics:
R2: 0.9323585033416748
MSE: 28926.00390625
MSE diff_p 27
----------
28: 0.0016951318830251694
Training Epoch 28
train_metrics: 
R2: 0.9691603779792786
MSE: 11101.9287109375
test metrics:
R2: 0.9552118182182312
MSE: 19258.759765625
MSE diff_p 28
----------
29: 0.0016224809223785996
Training Epoch 29
train_metrics: 
R2: 0.9712168574333191
MSE: 10626.115234375
test metrics:
R2: 0.9396481513977051
MSE: 26081.798828125
MSE diff_p 29
----------
30: 0.0015671985456719995
Training Epoch 30
train_metrics: 
R2: 0.9703192114830017
MSE: 10264.052734375
test metrics:
R2: 0.9517813920974731
MSE: 20224.896484375
MSE diff_p 30
----------
31: 0.0018328726291656494
Training Epoch 31
train_metrics: 
R2: 0.9693673849105835
MSE: 12004.0341796875
test metrics:
R2: 0.9218579530715942
MSE: 32157.41015625
MSE diff_p 31
----------
32: 0.0021402561105787754
Training Epoch 32
train_metrics: 
R2: 0.9576312899589539
MSE: 14017.181640625
test metrics:
R2: 0.9437295794487
MSE: 22550.3359375
MSE diff_p 32
----------
33: 0.0022942612413316965
Training Epoch 33
train_metrics: 
R2: 0.9616459012031555
MSE: 15025.806640625
test metrics:
R2: 0.9232122302055359
MSE: 31749.6796875
MSE diff_p 33
----------
34: 0.0022516720928251743
Training Epoch 34
train_metrics: 
R2: 0.9613109230995178
MSE: 14746.8779296875
test metrics:
R2: 0.9540536403656006
MSE: 19526.6640625
MSE diff_p 34
----------
35: 0.002462643664330244
Training Epoch 35
train_metrics: 
R2: 0.9538787603378296
MSE: 16128.591796875
test metrics:
R2: 0.9265395998954773
MSE: 31526.853515625
MSE diff_p 35
----------
36: 0.0029172878712415695
Training Epoch 36
train_metrics: 
R2: 0.9457672834396362
MSE: 19106.19921875
test metrics:
R2: 0.9510544538497925
MSE: 20677.962890625
MSE diff_p 36
----------
37: 0.003012214321643114
Training Epoch 37
train_metrics: 
R2: 0.952288806438446
MSE: 19727.8984375
test metrics:
R2: 0.9295611381530762
MSE: 31639.236328125
MSE diff_p 37
----------
38: 0.0026035436894744635
Training Epoch 38
train_metrics: 
R2: 0.9548506140708923
MSE: 17051.390625
test metrics:
R2: 0.9550846219062805
MSE: 19547.974609375
MSE diff_p 38
----------
39: 0.002372771967202425
Training Epoch 39
train_metrics: 
R2: 0.9566170573234558
MSE: 15539.9970703125
test metrics:
R2: 0.9343798756599426
MSE: 27910.369140625
MSE diff_p 39
----------
40: 0.0019946156535297632
Training Epoch 40
train_metrics: 
R2: 0.961007297039032
MSE: 13063.3369140625
test metrics:
R2: 0.9506591558456421
MSE: 20322.275390625
MSE diff_p 40
----------
41: 0.0017140175914391875
Training Epoch 41
train_metrics: 
R2: 0.968561053276062
MSE: 11225.6162109375
test metrics:
R2: 0.9453893303871155
MSE: 23217.439453125
MSE diff_p 41
----------
42: 0.0014895547647029161
Training Epoch 42
train_metrics: 
R2: 0.974107563495636
MSE: 9755.54296875
test metrics:
R2: 0.9530817270278931
MSE: 19523.201171875
MSE diff_p 42
----------
43: 0.0011591706424951553
Training Epoch 43
train_metrics: 
R2: 0.9796041250228882
MSE: 7591.7568359375
test metrics:
R2: 0.9500225782394409
MSE: 21147.30859375
MSE diff_p 43
----------
44: 0.001177930156700313
Training Epoch 44
train_metrics: 
R2: 0.980954110622406
MSE: 7714.6181640625
test metrics:
R2: 0.9544676542282104
MSE: 19392.0078125
MSE diff_p 44
----------
45: 0.0012628915719687939
Training Epoch 45
train_metrics: 
R2: 0.9771684408187866
MSE: 8271.056640625
test metrics:
R2: 0.9449894428253174
MSE: 22627.1640625
MSE diff_p 45
----------
46: 0.0010120616061612964
Training Epoch 46
train_metrics: 
R2: 0.9821987152099609
MSE: 6628.2958984375
test metrics:
R2: 0.9525737762451172
MSE: 19848.044921875
MSE diff_p 46
----------
47: 0.0013413135893642902
Training Epoch 47
train_metrics: 
R2: 0.9768880605697632
MSE: 8784.666015625
test metrics:
R2: 0.9491848945617676
MSE: 21598.955078125
MSE diff_p 47
----------
48: 0.001474107033573091
Training Epoch 48
train_metrics: 
R2: 0.9757364988327026
MSE: 9654.3701171875
test metrics:
R2: 0.9524829983711243
MSE: 19792.572265625
MSE diff_p 48
----------
49: 0.001872174791060388
Training Epoch 49
train_metrics: 
R2: 0.9652444124221802
MSE: 12261.435546875
test metrics:
R2: 0.9421148896217346
MSE: 24015.923828125
MSE diff_p 49
----------
50: 0.0021605915389955044
Training Epoch 50
train_metrics: 
R2: 0.9605847597122192
MSE: 14150.365234375
test metrics:
R2: 0.9501412510871887
MSE: 20525.46875
MSE diff_p 50
----------
51: 0.002672153525054455
Training Epoch 51
train_metrics: 
R2: 0.9575328230857849
MSE: 17500.740234375
test metrics:
R2: 0.9354214072227478
MSE: 27790.005859375
MSE diff_p 51
----------
52: 0.003070141188800335
Training Epoch 52
train_metrics: 
R2: 0.9393340945243835
MSE: 20107.27734375
test metrics:
R2: 0.9417732954025269
MSE: 23391.654296875
MSE diff_p 52
----------
53: 0.0030270263087004423
Training Epoch 53
train_metrics: 
R2: 0.9509250521659851
MSE: 19824.90625
test metrics:
R2: 0.9321631789207458
MSE: 29810.673828125
MSE diff_p 53
----------
54: 0.002407568274065852
Training Epoch 54
train_metrics: 
R2: 0.9558126926422119
MSE: 15767.888671875
test metrics:
R2: 0.9454976320266724
MSE: 21851.02734375
MSE diff_p 54
----------
55: 0.0022717551328241825
Training Epoch 55
train_metrics: 
R2: 0.9625275731086731
MSE: 14878.40625
test metrics:
R2: 0.9459040760993958
MSE: 23408.451171875
MSE diff_p 55
----------
56: 0.001777485478669405
Training Epoch 56
train_metrics: 
R2: 0.9690679311752319
MSE: 11641.287109375
test metrics:
R2: 0.9514725804328918
MSE: 19737.923828125
MSE diff_p 56
----------
57: 0.0015830621123313904
Training Epoch 57
train_metrics: 
R2: 0.9744434952735901
MSE: 10367.9501953125
test metrics:
R2: 0.9515821933746338
MSE: 21121.31640625
MSE diff_p 57
----------
58: 0.001708124065771699
Training Epoch 58
train_metrics: 
R2: 0.9697468876838684
MSE: 11187.017578125
test metrics:
R2: 0.9491920471191406
MSE: 20518.986328125
MSE diff_p 58
----------
59: 0.0016894250875338912
Training Epoch 59
train_metrics: 
R2: 0.9730523228645325
MSE: 11064.5537109375
test metrics:
R2: 0.9519449472427368
MSE: 20986.611328125
MSE diff_p 59
----------
60: 0.0019416756695136428
Training Epoch 60
train_metrics: 
R2: 0.9688488841056824
MSE: 12716.619140625
test metrics:
R2: 0.951686680316925
MSE: 19817.244140625
MSE diff_p 60
----------
61: 0.0015042643062770367
Training Epoch 61
train_metrics: 
R2: 0.974984884262085
MSE: 9851.880859375
test metrics:
R2: 0.9515687823295593
MSE: 20815.12109375
MSE diff_p 61
----------
62: 0.001590993721038103
Training Epoch 62
train_metrics: 
R2: 0.9718784093856812
MSE: 10419.896484375
test metrics:
R2: 0.9490009546279907
MSE: 20526.869140625
MSE diff_p 62
----------
63: 0.0015392197528854012
Training Epoch 63
train_metrics: 
R2: 0.9765139222145081
MSE: 10080.8134765625
test metrics:
R2: 0.9545276165008545
MSE: 19586.587890625
MSE diff_p 63
----------
64: 0.001501118065789342
Training Epoch 64
train_metrics: 
R2: 0.9731199741363525
MSE: 9831.2734375
test metrics:
R2: 0.9519679546356201
MSE: 19617.05078125
MSE diff_p 64
----------
65: 0.0019792928360402584
Training Epoch 65
train_metrics: 
R2: 0.9661745429039001
MSE: 12962.984375
test metrics:
R2: 0.9567230343818665
MSE: 19087.365234375
MSE diff_p 65
----------
66: 0.0012708611320704222
Training Epoch 66
train_metrics: 
R2: 0.9776642322540283
MSE: 8323.251953125
test metrics:
R2: 0.9515610933303833
MSE: 19746.830078125
MSE diff_p 66
----------
67: 0.001580911106429994
Training Epoch 67
train_metrics: 
R2: 0.9735342860221863
MSE: 10353.8623046875
test metrics:
R2: 0.9448296427726746
MSE: 22844.662109375
MSE diff_p 67
----------
68: 0.0017957227537408471
Training Epoch 68
train_metrics: 
R2: 0.9703639149665833
MSE: 11760.73046875
test metrics:
R2: 0.9544351100921631
MSE: 18893.93359375
MSE diff_p 68
----------
69: 0.0017936828080564737
Training Epoch 69
train_metrics: 
R2: 0.9672716856002808
MSE: 11747.369140625
test metrics:
R2: 0.9553362727165222
MSE: 19860.552734375
MSE diff_p 69
----------
70: 0.001280993572436273
Training Epoch 70
train_metrics: 
R2: 0.9778681993484497
MSE: 8389.61328125
test metrics:
R2: 0.9502646923065186
MSE: 20118.421875
MSE diff_p 70
----------
71: 0.0015669846907258034
Training Epoch 71
train_metrics: 
R2: 0.9752575159072876
MSE: 10262.6533203125
test metrics:
R2: 0.9449790120124817
MSE: 23039.017578125
MSE diff_p 71
----------
72: 0.0017234135884791613
Training Epoch 72
train_metrics: 
R2: 0.9703224897384644
MSE: 11287.154296875
test metrics:
R2: 0.9466319680213928
MSE: 21116.345703125
MSE diff_p 72
----------
73: 0.0023206297773867846
Training Epoch 73
train_metrics: 
R2: 0.956204354763031
MSE: 15198.5029296875
test metrics:
R2: 0.9440726637840271
MSE: 23707.580078125
MSE diff_p 73
----------
74: 0.001884101191535592
Training Epoch 74
train_metrics: 
R2: 0.9643515348434448
MSE: 12339.544921875
test metrics:
R2: 0.9552454948425293
MSE: 18705.552734375
MSE diff_p 74
----------
75: 0.002086331369355321
Training Epoch 75
train_metrics: 
R2: 0.9650258421897888
MSE: 13664.01171875
test metrics:
R2: 0.9471064209938049
MSE: 22902.92578125
MSE diff_p 75
----------
76: 0.002079363213852048
Training Epoch 76
train_metrics: 
R2: 0.9585453867912292
MSE: 13618.375
test metrics:
R2: 0.9486982226371765
MSE: 21119.130859375
MSE diff_p 76
----------
77: 0.002027579816058278
Training Epoch 77
train_metrics: 
R2: 0.9623017311096191
MSE: 13279.23046875
test metrics:
R2: 0.9444519877433777
MSE: 23869.82421875
MSE diff_p 77
----------
78: 0.0019808211363852024
Training Epoch 78
train_metrics: 
R2: 0.9658606052398682
MSE: 12972.9931640625
test metrics:
R2: 0.9407978057861328
MSE: 23027.693359375
MSE diff_p 78
----------
79: 0.0016320289578288794
Training Epoch 79
train_metrics: 
R2: 0.9771586060523987
MSE: 10688.6484375
test metrics:
R2: 0.9605634212493896
MSE: 18113.443359375
MSE diff_p 79
----------
80: 0.0011857757344841957
Training Epoch 80
train_metrics: 
R2: 0.9796422719955444
MSE: 7766.0009765625
test metrics:
R2: 0.9544393420219421
MSE: 18768.93359375
MSE diff_p 80
----------
81: 0.0009558403980918229
Training Epoch 81
train_metrics: 
R2: 0.9838207960128784
MSE: 6260.0859375
test metrics:
R2: 0.9545527696609497
MSE: 19192.77734375
MSE diff_p 81
----------
82: 0.0010506566613912582
Training Epoch 82
train_metrics: 
R2: 0.9799462556838989
MSE: 6881.0654296875
test metrics:
R2: 0.9529830813407898
MSE: 19208.490234375
MSE diff_p 82
----------
83: 0.0013661888660863042
Training Epoch 83
train_metrics: 
R2: 0.9773856401443481
MSE: 8947.580078125
test metrics:
R2: 0.9530963897705078
MSE: 19845.72265625
MSE diff_p 83
----------
84: 0.0013516561593860388
Training Epoch 84
train_metrics: 
R2: 0.971725583076477
MSE: 8852.4033203125
test metrics:
R2: 0.9641500115394592
MSE: 15641.1748046875
MSE diff_p 84
----------
85: 0.0011530111078172922
Training Epoch 85
train_metrics: 
R2: 0.9818567633628845
MSE: 7551.41552734375
test metrics:
R2: 0.954021692276001
MSE: 19882.212890625
MSE diff_p 85
----------
86: 0.0010255163069814444
Training Epoch 86
train_metrics: 
R2: 0.983966588973999
MSE: 6716.4150390625
test metrics:
R2: 0.9605252146720886
MSE: 16875.658203125
MSE diff_p 86
----------
87: 0.0012644589878618717
Training Epoch 87
train_metrics: 
R2: 0.9792150855064392
MSE: 8281.322265625
test metrics:
R2: 0.9644744992256165
MSE: 16092.474609375
MSE diff_p 87
----------
88: 0.001352740335278213
Training Epoch 88
train_metrics: 
R2: 0.9742701649665833
MSE: 8859.50390625
test metrics:
R2: 0.9582658410072327
MSE: 17516.599609375
MSE diff_p 88
----------
89: 0.0014331287238746881
Training Epoch 89
train_metrics: 
R2: 0.9744300842285156
MSE: 9385.9921875
test metrics:
R2: 0.9477963447570801
MSE: 21767.736328125
MSE diff_p 89
----------
90: 0.0017005830304697156
Training Epoch 90
train_metrics: 
R2: 0.9716182351112366
MSE: 11137.62890625
test metrics:
R2: 0.9585336446762085
MSE: 17475.90625
MSE diff_p 90
----------
91: 0.0018213754519820213
Training Epoch 91
train_metrics: 
R2: 0.9710510969161987
MSE: 11928.7333984375
test metrics:
R2: 0.9553310871124268
MSE: 20267.07421875
MSE diff_p 91
----------
92: 0.0024605835787951946
Training Epoch 92
train_metrics: 
R2: 0.9561766982078552
MSE: 16115.1025390625
test metrics:
R2: 0.9544097781181335
MSE: 18912.048828125
MSE diff_p 92
----------
93: 0.001695064827799797
Training Epoch 93
train_metrics: 
R2: 0.9758341312408447
MSE: 11101.4892578125
test metrics:
R2: 0.957508385181427
MSE: 19773.681640625
MSE diff_p 93
----------
94: 0.002012300305068493
Training Epoch 94
train_metrics: 
R2: 0.9636169672012329
MSE: 13179.16015625
test metrics:
R2: 0.9479256868362427
MSE: 20753.775390625
MSE diff_p 94
----------
95: 0.0018616837915033102
Training Epoch 95
train_metrics: 
R2: 0.9712406396865845
MSE: 12192.7265625
test metrics:
R2: 0.9525487422943115
MSE: 21399.25
MSE diff_p 95
----------
96: 0.0017472446197643876
Training Epoch 96
train_metrics: 
R2: 0.9668384790420532
MSE: 11443.228515625
test metrics:
R2: 0.9550683498382568
MSE: 18411.423828125
MSE diff_p 96
----------
97: 0.0013924234081059694
Training Epoch 97
train_metrics: 
R2: 0.9759819507598877
MSE: 9119.400390625
test metrics:
R2: 0.9562052488327026
MSE: 19425.099609375
MSE diff_p 97
----------
98: 0.0012903496390208602
Training Epoch 98
train_metrics: 
R2: 0.9790844917297363
MSE: 8450.88671875
test metrics:
R2: 0.9618093371391296
MSE: 16350.771484375
MSE diff_p 98
----------
99: 0.0016349087236449122
Training Epoch 99
train_metrics: 
R2: 0.9741588830947876
MSE: 10707.5078125
test metrics:
R2: 0.9527885913848877
MSE: 20520.203125
MSE diff_p 99
----------
100: 0.0018503485480323434
Training Epoch 100
train_metrics: 
R2: 0.9614772200584412
MSE: 12118.48828125
test metrics:
R2: 0.953974187374115
MSE: 18844.240234375
MSE diff_p 100
----------
101: 0.0015730797313153744
Training Epoch 101
train_metrics: 
R2: 0.9734334349632263
MSE: 10302.5712890625
test metrics:
R2: 0.9502209424972534
MSE: 21335.662109375
MSE diff_p 101
----------
102: 0.0011261843610554934
Training Epoch 102
train_metrics: 
R2: 0.9813011288642883
MSE: 7375.7197265625
test metrics:
R2: 0.963175356388092
MSE: 15968.10546875
MSE diff_p 102
----------
103: 0.0014559219125658274
Training Epoch 103
train_metrics: 
R2: 0.9744254350662231
MSE: 9535.2685546875
test metrics:
R2: 0.9596269726753235
MSE: 17633.6640625
MSE diff_p 103
----------
104: 0.0009267239365726709
Training Epoch 104
train_metrics: 
R2: 0.9807271361351013
MSE: 6069.392578125
test metrics:
R2: 0.9614080786705017
MSE: 16425.1875
MSE diff_p 104
----------
105: 0.0011167747434228659
Training Epoch 105
train_metrics: 
R2: 0.9811006784439087
MSE: 7314.09375
test metrics:
R2: 0.9636891484260559
MSE: 16387.998046875
MSE diff_p 105
----------
106: 0.0014103660359978676
Training Epoch 106
train_metrics: 
R2: 0.974402129650116
MSE: 9236.91015625
test metrics:
R2: 0.9588994383811951
MSE: 17162.6171875
MSE diff_p 106
----------
107: 0.0018792061600834131
Training Epoch 107
train_metrics: 
R2: 0.9679374098777771
MSE: 12307.486328125
test metrics:
R2: 0.9607587456703186
MSE: 17592.03125
MSE diff_p 107
----------
108: 0.0017480830429121852
Training Epoch 108
train_metrics: 
R2: 0.9665753841400146
MSE: 11448.720703125
test metrics:
R2: 0.9566864967346191
MSE: 17935.70703125
MSE diff_p 108
----------
109: 0.0015443176962435246
Training Epoch 109
train_metrics: 
R2: 0.9766827821731567
MSE: 10114.203125
test metrics:
R2: 0.9592654705047607
MSE: 18689.7109375
MSE diff_p 109
----------
110: 0.001814920688048005
Training Epoch 110
train_metrics: 
R2: 0.9691464304924011
MSE: 11886.462890625
test metrics:
R2: 0.9575647711753845
MSE: 17629.19921875
MSE diff_p 110
----------
111: 0.0013909917324781418
Training Epoch 111
train_metrics: 
R2: 0.9744076728820801
MSE: 9110.021484375
test metrics:
R2: 0.9514074921607971
MSE: 20872.939453125
MSE diff_p 111
----------
112: 0.001935832668095827
Training Epoch 112
train_metrics: 
R2: 0.9662086367607117
MSE: 12678.349609375
test metrics:
R2: 0.954925537109375
MSE: 18636.416015625
MSE diff_p 112
----------
113: 0.0017883437685668468
Training Epoch 113
train_metrics: 
R2: 0.9671962857246399
MSE: 11712.4013671875
test metrics:
R2: 0.9472939968109131
MSE: 22663.84765625
MSE diff_p 113
----------
114: 0.0019225814612582326
Training Epoch 114
train_metrics: 
R2: 0.9630225300788879
MSE: 12591.56640625
test metrics:
R2: 0.9528868794441223
MSE: 19299.583984375
MSE diff_p 114
----------
115: 0.0018814927898347378
Training Epoch 115
train_metrics: 
R2: 0.9717381000518799
MSE: 12322.4609375
test metrics:
R2: 0.9607441425323486
MSE: 18464.25
MSE diff_p 115
----------
116: 0.0015955462586134672
Training Epoch 116
train_metrics: 
R2: 0.9702543616294861
MSE: 10449.7119140625
test metrics:
R2: 0.962401807308197
MSE: 16164.23828125
MSE diff_p 116
----------
117: 0.001272967318072915
Training Epoch 117
train_metrics: 
R2: 0.978236973285675
MSE: 8337.0458984375
test metrics:
R2: 0.9625096321105957
MSE: 16870.646484375
MSE diff_p 117
----------
118: 0.0011036634678021073
Training Epoch 118
train_metrics: 
R2: 0.9805788397789001
MSE: 7228.2236328125
test metrics:
R2: 0.9611595273017883
MSE: 16416.4765625
MSE diff_p 118
----------
119: 0.0008362930966541171
Training Epoch 119
train_metrics: 
R2: 0.9858087301254272
MSE: 5477.134765625
test metrics:
R2: 0.9656779170036316
MSE: 15291.998046875
MSE diff_p 119
----------
120: 0.0008742365753278136
Training Epoch 120
train_metrics: 
R2: 0.9852887392044067
MSE: 5725.6376953125
test metrics:
R2: 0.9649121761322021
MSE: 15130.974609375
MSE diff_p 120
----------
121: 0.001246658037416637
Training Epoch 121
train_metrics: 
R2: 0.9790818095207214
MSE: 8164.740234375
test metrics:
R2: 0.9646944403648376
MSE: 15403.86328125
MSE diff_p 121
----------
122: 0.001153306569904089
Training Epoch 122
train_metrics: 
R2: 0.980269193649292
MSE: 7553.3515625
test metrics:
R2: 0.9671035408973694
MSE: 14508.2568359375
MSE diff_p 122
----------
123: 0.0011344145750626922
Training Epoch 123
train_metrics: 
R2: 0.9797363877296448
MSE: 7429.6220703125
test metrics:
R2: 0.9657489061355591
MSE: 14842.087890625
MSE diff_p 123
----------
124: 0.0011690128594636917
Training Epoch 124
train_metrics: 
R2: 0.982871413230896
MSE: 7656.21826171875
test metrics:
R2: 0.9723762273788452
MSE: 12823.5537109375
MSE diff_p 124
----------
125: 0.0010044884402304888
Training Epoch 125
train_metrics: 
R2: 0.9812746047973633
MSE: 6578.697265625
test metrics:
R2: 0.9640489220619202
MSE: 15460.806640625
MSE diff_p 125
----------
126: 0.0008213439723476768
Training Epoch 126
train_metrics: 
R2: 0.9870088696479797
MSE: 5379.2294921875
test metrics:
R2: 0.9659788608551025
MSE: 14952.15625
MSE diff_p 126
----------
127: 0.0008465339778922498
Training Epoch 127
train_metrics: 
R2: 0.9850165843963623
MSE: 5544.205078125
test metrics:
R2: 0.9641420841217041
MSE: 15406.0166015625
MSE diff_p 127
----------
128: 0.001006359583698213
Training Epoch 128
train_metrics: 
R2: 0.981167197227478
MSE: 6590.9521484375
test metrics:
R2: 0.966163158416748
MSE: 14972.724609375
MSE diff_p 128
----------
129: 0.0009738310473039746
Training Epoch 129
train_metrics: 
R2: 0.982456624507904
MSE: 6377.91259765625
test metrics:
R2: 0.9633182287216187
MSE: 15578.6435546875
MSE diff_p 129
----------
130: 0.0014375956961885095
Training Epoch 130
train_metrics: 
R2: 0.9763152599334717
MSE: 9415.2470703125
test metrics:
R2: 0.9665165543556213
MSE: 15373.10546875
MSE diff_p 130
----------
131: 0.0018261008663102984
Training Epoch 131
train_metrics: 
R2: 0.9595639109611511
MSE: 11959.68359375
test metrics:
R2: 0.946435272693634
MSE: 21189.693359375
MSE diff_p 131
----------
132: 0.0029866190161556005
Training Epoch 132
train_metrics: 
R2: 0.9501470327377319
MSE: 19560.265625
test metrics:
R2: 0.9503641128540039
MSE: 22446.63671875
MSE diff_p 132
----------
133: 0.00366763724014163
Training Epoch 133
train_metrics: 
R2: 0.9343434572219849
MSE: 24020.4609375
test metrics:
R2: 0.9465857744216919
MSE: 23343.0390625
MSE diff_p 133
----------
134: 0.004440145567059517
Training Epoch 134
train_metrics: 
R2: 0.9297524690628052
MSE: 29079.84765625
test metrics:
R2: 0.9507290720939636
MSE: 24173.767578125
MSE diff_p 134
----------
135: 0.003245452418923378
Training Epoch 135
train_metrics: 
R2: 0.9355562329292297
MSE: 21255.4453125
test metrics:
R2: 0.9280218482017517
MSE: 27936.904296875
MSE diff_p 135
----------
136: 0.0018296604976058006
Training Epoch 136
train_metrics: 
R2: 0.9664744734764099
MSE: 11982.9970703125
test metrics:
R2: 0.9392949342727661
MSE: 24344.826171875
MSE diff_p 136
----------
137: 0.0020762195345014334
Training Epoch 137
train_metrics: 
R2: 0.9667084217071533
MSE: 13597.7861328125
test metrics:
R2: 0.9625958204269409
MSE: 16616.376953125
MSE diff_p 137
----------
138: 0.001957597676664591
Training Epoch 138
train_metrics: 
R2: 0.9655161499977112
MSE: 12820.896484375
test metrics:
R2: 0.9496514797210693
MSE: 20531.595703125
MSE diff_p 138
----------
139: 0.0012007949408143759
Training Epoch 139
train_metrics: 
R2: 0.9822137355804443
MSE: 7864.36669921875
test metrics:
R2: 0.9674046039581299
MSE: 14661.9931640625
MSE diff_p 139
----------
140: 0.0011562713189050555
Training Epoch 140
train_metrics: 
R2: 0.9814465045928955
MSE: 7572.7685546875
test metrics:
R2: 0.9554611444473267
MSE: 18564.759765625
MSE diff_p 140
----------
141: 0.0011627399362623692
Training Epoch 141
train_metrics: 
R2: 0.9813770055770874
MSE: 7615.1328125
test metrics:
R2: 0.963100790977478
MSE: 15715.5771484375
MSE diff_p 141
----------
142: 0.0011374810710549355
Training Epoch 142
train_metrics: 
R2: 0.9826999306678772
MSE: 7449.70703125
test metrics:
R2: 0.9660558700561523
MSE: 14961.615234375
MSE diff_p 142
----------
143: 0.0007639214745722711
Training Epoch 143
train_metrics: 
R2: 0.9873831272125244
MSE: 5003.1513671875
test metrics:
R2: 0.9630701541900635
MSE: 15666.466796875
MSE diff_p 143
----------
144: 0.0011491032782942057
Training Epoch 144
train_metrics: 
R2: 0.9821987748146057
MSE: 7525.8232421875
test metrics:
R2: 0.9681490063667297
MSE: 14047.603515625
MSE diff_p 144
----------
145: 0.0011447572614997625
Training Epoch 145
train_metrics: 
R2: 0.978346586227417
MSE: 7497.3603515625
test metrics:
R2: 0.9645882248878479
MSE: 15392.046875
MSE diff_p 145
----------
146: 0.0006846222677268088
Training Epoch 146
train_metrics: 
R2: 0.986611545085907
MSE: 4483.796875
test metrics:
R2: 0.9693589806556702
MSE: 13572.3447265625
MSE diff_p 146
----------
147: 0.0008275539730675519
Training Epoch 147
train_metrics: 
R2: 0.9856740236282349
MSE: 5419.900390625
test metrics:
R2: 0.9704121947288513
MSE: 13282.2685546875
MSE diff_p 147
----------
148: 0.0009736904758028686
Training Epoch 148
train_metrics: 
R2: 0.9810811281204224
MSE: 6376.9912109375
test metrics:
R2: 0.9669954776763916
MSE: 14654.044921875
MSE diff_p 148
----------
149: 0.0010612537153065205
Training Epoch 149
train_metrics: 
R2: 0.9805027842521667
MSE: 6950.47021484375
test metrics:
R2: 0.9685540199279785
MSE: 13854.029296875
MSE diff_p 149
----------
150: 0.0009641177603043616
Training Epoch 150
train_metrics: 
R2: 0.9833061099052429
MSE: 6314.29736328125
test metrics:
R2: 0.9672168493270874
MSE: 14441.478515625
MSE diff_p 150
----------
151: 0.0010209653992205858
Training Epoch 151
train_metrics: 
R2: 0.9821451902389526
MSE: 6686.60888671875
test metrics:
R2: 0.9648711681365967
MSE: 15099.06640625
MSE diff_p 151
----------
152: 0.0008459900273010135
Training Epoch 152
train_metrics: 
R2: 0.9854608774185181
MSE: 5540.64208984375
test metrics:
R2: 0.9678761959075928
MSE: 14093.9580078125
MSE diff_p 152
----------
153: 0.0013251369819045067
Training Epoch 153
train_metrics: 
R2: 0.9785434007644653
MSE: 8678.7216796875
test metrics:
R2: 0.9713362455368042
MSE: 12910.76953125
MSE diff_p 153
----------
154: 0.0007822221959941089
Training Epoch 154
train_metrics: 
R2: 0.9860411286354065
MSE: 5123.0087890625
test metrics:
R2: 0.9674878120422363
MSE: 14206.0732421875
MSE diff_p 154
----------
155: 0.0008710232796147466
Training Epoch 155
train_metrics: 
R2: 0.984721839427948
MSE: 5704.59326171875
test metrics:
R2: 0.9684767723083496
MSE: 13806.5390625
MSE diff_p 155
----------
156: 0.0007567855645902455
Training Epoch 156
train_metrics: 
R2: 0.9872579574584961
MSE: 4956.4169921875
test metrics:
R2: 0.9743683338165283
MSE: 12011.212890625
MSE diff_p 156
----------
157: 0.001154949190095067
Training Epoch 157
train_metrics: 
R2: 0.9798334836959839
MSE: 7564.109375
test metrics:
R2: 0.9662463665008545
MSE: 14429.599609375
MSE diff_p 157
----------
158: 0.0012480315053835511
Training Epoch 158
train_metrics: 
R2: 0.9813653230667114
MSE: 8173.73388671875
test metrics:
R2: 0.9751416444778442
MSE: 12030.544921875
MSE diff_p 158
----------
159: 0.0016837193397805095
Training Epoch 159
train_metrics: 
R2: 0.9715715646743774
MSE: 11027.18359375
test metrics:
R2: 0.953640878200531
MSE: 18481.630859375
MSE diff_p 159
----------
160: 0.0021156698931008577
Training Epoch 160
train_metrics: 
R2: 0.970544695854187
MSE: 13856.1591796875
test metrics:
R2: 0.9716982245445251
MSE: 14033.9248046875
MSE diff_p 160
----------
161: 0.0021702321246266365
Training Epoch 161
train_metrics: 
R2: 0.9489912986755371
MSE: 14213.50390625
test metrics:
R2: 0.9408228397369385
MSE: 22860.728515625
MSE diff_p 161
----------
162: 0.0031641856767237186
Training Epoch 162
train_metrics: 
R2: 0.9449061751365662
MSE: 20723.203125
test metrics:
R2: 0.9440566897392273
MSE: 24532.759765625
MSE diff_p 162
----------
163: 0.004110002890229225
Training Epoch 163
train_metrics: 
R2: 0.9325920939445496
MSE: 26917.64453125
test metrics:
R2: 0.946464478969574
MSE: 25441.9609375
MSE diff_p 163
----------
164: 0.0027338434010744095
Training Epoch 164
train_metrics: 
R2: 0.9538624882698059
MSE: 17904.765625
test metrics:
R2: 0.9510725140571594
MSE: 21555.6875
MSE diff_p 164
----------
165: 0.0028616301715373993
Training Epoch 165
train_metrics: 
R2: 0.9393120408058167
MSE: 18741.677734375
test metrics:
R2: 0.9550185799598694
MSE: 19297.03515625
MSE diff_p 165
----------
166: 0.0022253384813666344
Training Epoch 166
train_metrics: 
R2: 0.9636097550392151
MSE: 14574.4111328125
test metrics:
R2: 0.9583278298377991
MSE: 18089.044921875
MSE diff_p 166
----------
167: 0.0016313587548211217
Training Epoch 167
train_metrics: 
R2: 0.9675828218460083
MSE: 10684.2587890625
test metrics:
R2: 0.9463354349136353
MSE: 21117.451171875
MSE diff_p 167
----------
168: 0.0017207185737788677
Training Epoch 168
train_metrics: 
R2: 0.9759036302566528
MSE: 11269.50390625
test metrics:
R2: 0.969630777835846
MSE: 14145.162109375
MSE diff_p 168
----------
169: 0.0009586259257048368
Training Epoch 169
train_metrics: 
R2: 0.9787617921829224
MSE: 6278.330078125
test metrics:
R2: 0.9368129372596741
MSE: 23349.880859375
MSE diff_p 169
----------
170: 0.0012125162174925208
Training Epoch 170
train_metrics: 
R2: 0.980563223361969
MSE: 7941.13330078125
test metrics:
R2: 0.9711209535598755
MSE: 13327.263671875
MSE diff_p 170
----------
171: 0.0010991254821419716
Training Epoch 171
train_metrics: 
R2: 0.9767710566520691
MSE: 7198.50244140625
test metrics:
R2: 0.9430711269378662
MSE: 21656.349609375
MSE diff_p 171
----------
172: 0.0009882780723273754
Training Epoch 172
train_metrics: 
R2: 0.9838088154792786
MSE: 6472.5302734375
test metrics:
R2: 0.9728922247886658
MSE: 12645.6484375
MSE diff_p 172
----------
173: 0.0011890246532857418
Training Epoch 173
train_metrics: 
R2: 0.9790130257606506
MSE: 7787.2783203125
test metrics:
R2: 0.9525859355926514
MSE: 18850.373046875
MSE diff_p 173
----------
174: 0.0011767857940867543
Training Epoch 174
train_metrics: 
R2: 0.9803910255432129
MSE: 7707.12353515625
test metrics:
R2: 0.9755072593688965
MSE: 11740.67578125
MSE diff_p 174
----------
175: 0.0007156629581004381
Training Epoch 175
train_metrics: 
R2: 0.9885230660438538
MSE: 4687.09228515625
test metrics:
R2: 0.9659538269042969
MSE: 14555.2548828125
MSE diff_p 175
----------
176: 0.0008616315899416804
Training Epoch 176
train_metrics: 
R2: 0.9865990281105042
MSE: 5643.083984375
test metrics:
R2: 0.9702020287513733
MSE: 13380.169921875
MSE diff_p 176
----------
177: 0.0007808765512891114
Training Epoch 177
train_metrics: 
R2: 0.9863556623458862
MSE: 5114.19580078125
test metrics:
R2: 0.966065526008606
MSE: 14577.0166015625
MSE diff_p 177
----------
178: 0.0009773543570190668
Training Epoch 178
train_metrics: 
R2: 0.9810945987701416
MSE: 6400.98828125
test metrics:
R2: 0.9699809551239014
MSE: 13405.0029296875
MSE diff_p 178
----------
179: 0.0009205759852193296
Training Epoch 179
train_metrics: 
R2: 0.9791380763053894
MSE: 6029.12939453125
test metrics:
R2: 0.9647934436798096
MSE: 15014.3779296875
MSE diff_p 179
----------
180: 0.0009170587291009724
Training Epoch 180
train_metrics: 
R2: 0.9833205938339233
MSE: 6006.0927734375
test metrics:
R2: 0.9660908579826355
MSE: 14817.9833984375
MSE diff_p 180
----------
181: 0.001124011934734881
Training Epoch 181
train_metrics: 
R2: 0.9820067882537842
MSE: 7361.49169921875
test metrics:
R2: 0.9643340706825256
MSE: 15279.7080078125
MSE diff_p 181
----------
182: 0.0010810275562107563
Training Epoch 182
train_metrics: 
R2: 0.9814567565917969
MSE: 7079.97509765625
test metrics:
R2: 0.9612566232681274
MSE: 16644.03515625
MSE diff_p 182
----------
183: 0.001357200788334012
Training Epoch 183
train_metrics: 
R2: 0.9759888052940369
MSE: 8888.716796875
test metrics:
R2: 0.9642080068588257
MSE: 16022.990234375
MSE diff_p 183
----------
184: 0.0015462945448234677
Training Epoch 184
train_metrics: 
R2: 0.9721481800079346
MSE: 10127.146484375
test metrics:
R2: 0.9568141102790833
MSE: 18553.35546875
MSE diff_p 184
----------
185: 0.0018371082842350006
Training Epoch 185
train_metrics: 
R2: 0.9690302610397339
MSE: 12031.7744140625
test metrics:
R2: 0.9619404077529907
MSE: 16629.98046875
MSE diff_p 185
----------
186: 0.0019158871145918965
Training Epoch 186
train_metrics: 
R2: 0.9689973592758179
MSE: 12547.720703125
test metrics:
R2: 0.9656028747558594
MSE: 15749.693359375
MSE diff_p 186
----------
187: 0.0020437128841876984
Training Epoch 187
train_metrics: 
R2: 0.9628126621246338
MSE: 13384.890625
test metrics:
R2: 0.9493414163589478
MSE: 20456.61328125
MSE diff_p 187
----------
188: 0.0019140513613820076
Training Epoch 188
train_metrics: 
R2: 0.9720091223716736
MSE: 12535.6982421875
test metrics:
R2: 0.9626644253730774
MSE: 17113.830078125
MSE diff_p 188
----------
189: 0.0018534280825406313
Training Epoch 189
train_metrics: 
R2: 0.9630136489868164
MSE: 12138.658203125
test metrics:
R2: 0.9537680745124817
MSE: 18690.486328125
MSE diff_p 189
----------
190: 0.0015348889864981174
Training Epoch 190
train_metrics: 
R2: 0.9751833081245422
MSE: 10052.4501953125
test metrics:
R2: 0.9620352387428284
MSE: 16666.470703125
MSE diff_p 190
----------
191: 0.0022056654561311007
Training Epoch 191
train_metrics: 
R2: 0.9618064165115356
MSE: 14445.568359375
test metrics:
R2: 0.9555723071098328
MSE: 17985.3359375
MSE diff_p 191
----------
192: 0.0014312639832496643
Training Epoch 192
train_metrics: 
R2: 0.9734703898429871
MSE: 9373.7783203125
test metrics:
R2: 0.9635275602340698
MSE: 16141.34375
MSE diff_p 192
----------
193: 0.0012471020454540849
Training Epoch 193
train_metrics: 
R2: 0.97843337059021
MSE: 8167.646484375
test metrics:
R2: 0.9639698266983032
MSE: 15445.4248046875
MSE diff_p 193
----------
194: 0.0015664433594793081
Training Epoch 194
train_metrics: 
R2: 0.9714003205299377
MSE: 10259.109375
test metrics:
R2: 0.9533425569534302
MSE: 19086.779296875
MSE diff_p 194
----------
195: 0.0013097398914396763
Training Epoch 195
train_metrics: 
R2: 0.9762807488441467
MSE: 8577.87890625
test metrics:
R2: 0.967140257358551
MSE: 14868.0166015625
MSE diff_p 195
----------
196: 0.001167987589724362
Training Epoch 196
train_metrics: 
R2: 0.9811500906944275
MSE: 7649.50146484375
test metrics:
R2: 0.9470950365066528
MSE: 21172.783203125
MSE diff_p 196
----------
197: 0.0015147285303100944
Training Epoch 197
train_metrics: 
R2: 0.9755097031593323
MSE: 9920.412109375
test metrics:
R2: 0.96480792760849
MSE: 15454.9833984375
MSE diff_p 197
----------
198: 0.0011612216476351023
Training Epoch 198
train_metrics: 
R2: 0.9807825088500977
MSE: 7605.189453125
test metrics:
R2: 0.9682825803756714
MSE: 13999.5537109375
MSE diff_p 198
----------
199: 0.0012476458214223385
Training Epoch 199
train_metrics: 
R2: 0.9751301407814026
MSE: 8171.20849609375
test metrics:
R2: 0.9439347982406616
MSE: 21268.572265625
MSE diff_p 199
----------
