==========================================
SLURM_CLUSTER_NAME = param-shivay
SLURM_JOB_ACCOUNT = comp_sci_engg
SLURM_JOB_ID = 1018380
SLURM_JOB_NAME = printjob
SLURM_JOB_NODELIST = cn013
SLURM_JOB_USER = soumyadeep.das.cse21.itbhu
SLURM_JOB_UID = 5621
SLURM_JOB_PARTITION = cpu
SLURM_TASK_PID = 34050
SLURM_SUBMIT_DIR = /home/soumyadeep.das.cse21.itbhu/quantum-ml/LexiQL
SLURM_CPUS_ON_NODE = 5
SLURM_NTASKS = 
SLURM_TASK_PID = 34050
==========================================
hello world
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                40
On-line CPU(s) list:   0-39
Thread(s) per core:    1
Core(s) per socket:    20
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 85
Model name:            Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz
Stepping:              4
CPU MHz:               2400.000
BogoMIPS:              4800.00
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              1024K
L3 cache:              28160K
NUMA node0 CPU(s):     0-19
NUMA node1 CPU(s):     20-39
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_ppin intel_pt ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke spec_ctrl intel_stibp flush_l1d
              total        used        free      shared  buff/cache   available
Mem:           187G        6.2G        167G        3.6G         13G        175G
Swap:            0B          0B          0B
module loaded
env loaded
python is running
args.num_latent=3
args.num_trash=7

 Training Encoder...

Encoder Loss: 0.182 Iteration: 1
Encoder Loss: 0.11 Iteration: 2
Encoder Loss: 0.066 Iteration: 3
Encoder Loss: 0.062 Iteration: 4
Encoder Loss: 0.061 Iteration: 5
Encoder Loss: 0.055 Iteration: 6
Encoder Loss: 0.052 Iteration: 7
Encoder Loss: 0.055 Iteration: 8
Encoder Loss: 0.053 Iteration: 9
Encoder Loss: 0.058 Iteration: 10
Encoder Loss: 0.054 Iteration: 11
Encoder Loss: 0.057 Iteration: 12
Encoder Loss: 0.056 Iteration: 13
Encoder Loss: 0.06 Iteration: 14
Encoder Loss: 0.057 Iteration: 15
Encoder Loss: 0.055 Iteration: 16
Encoder Loss: 0.062 Iteration: 17
Encoder Loss: 0.059 Iteration: 18
Encoder Loss: 0.06 Iteration: 19
Encoder Loss: 0.051 Iteration: 20
Encoder Loss: 0.053 Iteration: 21
Encoder Loss: 0.056 Iteration: 22
Encoder Loss: 0.058 Iteration: 23
Encoder Loss: 0.052 Iteration: 24
Encoder Loss: 0.058 Iteration: 25
Encoder Loss: 0.062 Iteration: 26
Encoder Loss: 0.057 Iteration: 27
Encoder Loss: 0.055 Iteration: 28
Encoder Loss: 0.059 Iteration: 29
Encoder Loss: 0.057 Iteration: 30
Encoder Loss: 0.06 Iteration: 31
Encoder Loss: 0.056 Iteration: 32
Encoder Loss: 0.058 Iteration: 33
Encoder Loss: 0.05 Iteration: 34
Encoder Loss: 0.056 Iteration: 35
Encoder Loss: 0.058 Iteration: 36
Encoder Loss: 0.054 Iteration: 37
Encoder Loss: 0.057 Iteration: 38
Encoder Loss: 0.054 Iteration: 39
Encoder Loss: 0.059 Iteration: 40
Encoder Loss: 0.058 Iteration: 41
Encoder Loss: 0.065 Iteration: 42
Encoder Loss: 0.055 Iteration: 43
Encoder Loss: 0.056 Iteration: 44
Encoder Loss: 0.06 Iteration: 45
Encoder Loss: 0.061 Iteration: 46
Encoder Loss: 0.056 Iteration: 47
Encoder Loss: 0.06 Iteration: 48
Encoder Loss: 0.054 Iteration: 49
Encoder Loss: 0.062 Iteration: 50
Encoder Loss: 0.055 Iteration: 51
Encoder Loss: 0.055 Iteration: 52
Encoder Loss: 0.052 Iteration: 53
Encoder Loss: 0.051 Iteration: 54
Encoder Loss: 0.046 Iteration: 55
Encoder Loss: 0.052 Iteration: 56
Encoder Loss: 0.06 Iteration: 57
Encoder Loss: 0.054 Iteration: 58
Encoder Loss: 0.053 Iteration: 59
Encoder Loss: 0.057 Iteration: 60
Encoder Loss: 0.052 Iteration: 61
Encoder Loss: 0.053 Iteration: 62
Encoder Loss: 0.055 Iteration: 63
Encoder Loss: 0.054 Iteration: 64
Encoder Loss: 0.052 Iteration: 65
Encoder Loss: 0.054 Iteration: 66
Encoder Loss: 0.059 Iteration: 67
Encoder Loss: 0.056 Iteration: 68
Encoder Loss: 0.061 Iteration: 69
Encoder Loss: 0.062 Iteration: 70
Encoder Loss: 0.053 Iteration: 71
Encoder Loss: 0.057 Iteration: 72
Encoder Loss: 0.051 Iteration: 73
Encoder Loss: 0.055 Iteration: 74
Encoder Loss: 0.058 Iteration: 75
Encoder Loss: 0.054 Iteration: 76
Encoder Loss: 0.058 Iteration: 77
Encoder Loss: 0.053 Iteration: 78
Encoder Loss: 0.062 Iteration: 79
Encoder Loss: 0.06 Iteration: 80
Encoder Loss: 0.053 Iteration: 81
Encoder Loss: 0.054 Iteration: 82
Encoder Loss: 0.057 Iteration: 83
Encoder Loss: 0.062 Iteration: 84
Encoder Loss: 0.058 Iteration: 85
Encoder Loss: 0.058 Iteration: 86
Encoder Loss: 0.06 Iteration: 87
Encoder Loss: 0.061 Iteration: 88
Encoder Loss: 0.055 Iteration: 89
Encoder Loss: 0.061 Iteration: 90
Encoder Loss: 0.059 Iteration: 91
Encoder Loss: 0.058 Iteration: 92
Encoder Loss: 0.056 Iteration: 93
Encoder Loss: 0.055 Iteration: 94
Encoder Loss: 0.061 Iteration: 95
Encoder Loss: 0.057 Iteration: 96
Encoder Loss: 0.057 Iteration: 97
Encoder Loss: 0.054 Iteration: 98
Encoder Loss: 0.06 Iteration: 99
Encoder Loss: 0.056 Iteration: 100
Encoder Loss: 0.057 Iteration: 101
Encoder Loss: 0.057 Iteration: 102
Encoder Loss: 0.055 Iteration: 103
Encoder Loss: 0.052 Iteration: 104
Encoder Loss: 0.06 Iteration: 105
Encoder Loss: 0.055 Iteration: 106
Encoder Loss: 0.048 Iteration: 107
Encoder Loss: 0.055 Iteration: 108
Encoder Loss: 0.056 Iteration: 109
Encoder Loss: 0.053 Iteration: 110
Encoder Loss: 0.059 Iteration: 111
Encoder Loss: 0.057 Iteration: 112
Encoder Loss: 0.054 Iteration: 113
Encoder Loss: 0.055 Iteration: 114
Encoder Loss: 0.059 Iteration: 115
Encoder Loss: 0.055 Iteration: 116
Encoder Loss: 0.058 Iteration: 117
Encoder Loss: 0.053 Iteration: 118
Encoder Loss: 0.06 Iteration: 119
Encoder Loss: 0.058 Iteration: 120
Encoder Loss: 0.055 Iteration: 121
Encoder Loss: 0.058 Iteration: 122
Encoder Loss: 0.056 Iteration: 123
Encoder Loss: 0.053 Iteration: 124
Encoder Loss: 0.052 Iteration: 125
Encoder Loss: 0.059 Iteration: 126
Encoder Loss: 0.055 Iteration: 127
Encoder Loss: 0.058 Iteration: 128
Encoder Loss: 0.057 Iteration: 129
Encoder Loss: 0.057 Iteration: 130
Encoder Loss: 0.058 Iteration: 131
Encoder Loss: 0.06 Iteration: 132
Encoder Loss: 0.061 Iteration: 133
Encoder Loss: 0.061 Iteration: 134
Encoder Loss: 0.053 Iteration: 135
Encoder Loss: 0.059 Iteration: 136
Encoder Loss: 0.057 Iteration: 137
Encoder Loss: 0.058 Iteration: 138
Encoder Loss: 0.052 Iteration: 139
Encoder Loss: 0.059 Iteration: 140
Encoder Loss: 0.055 Iteration: 141
Encoder Loss: 0.053 Iteration: 142
Encoder Loss: 0.058 Iteration: 143
Encoder Loss: 0.053 Iteration: 144
Encoder Loss: 0.057 Iteration: 145
Encoder Loss: 0.059 Iteration: 146
Encoder Loss: 0.052 Iteration: 147
Encoder Loss: 0.056 Iteration: 148
Encoder Loss: 0.052 Iteration: 149
Encoder Loss: 0.052 Iteration: 150
Encoder Loss: 0.054 Iteration: 151
Encoder Loss: 0.058 Iteration: 152
Encoder Loss: 0.055 Iteration: 153
Encoder Loss: 0.054 Iteration: 154
Encoder Loss: 0.046 Iteration: 155
Encoder Loss: 0.054 Iteration: 156
Encoder Loss: 0.06 Iteration: 157
Encoder Loss: 0.057 Iteration: 158
Encoder Loss: 0.058 Iteration: 159
Encoder Loss: 0.059 Iteration: 160
Encoder Loss: 0.053 Iteration: 161
Encoder Loss: 0.047 Iteration: 162
Encoder Loss: 0.062 Iteration: 163
Encoder Loss: 0.055 Iteration: 164
Encoder Loss: 0.056 Iteration: 165
Encoder Loss: 0.058 Iteration: 166
Encoder Loss: 0.058 Iteration: 167
Encoder Loss: 0.06 Iteration: 168
Encoder Loss: 0.05 Iteration: 169
Encoder Loss: 0.053 Iteration: 170
Encoder Loss: 0.057 Iteration: 171
Encoder Loss: 0.056 Iteration: 172
Encoder Loss: 0.058 Iteration: 173
Encoder Loss: 0.054 Iteration: 174
Encoder Loss: 0.06 Iteration: 175
Encoder Loss: 0.053 Iteration: 176
Encoder Loss: 0.062 Iteration: 177
Encoder Loss: 0.055 Iteration: 178
Encoder Loss: 0.06 Iteration: 179
Encoder Loss: 0.056 Iteration: 180
Encoder Loss: 0.055 Iteration: 181
Encoder Loss: 0.054 Iteration: 182
Encoder Loss: 0.052 Iteration: 183
Encoder Loss: 0.062 Iteration: 184
Encoder Loss: 0.057 Iteration: 185
Encoder Loss: 0.058 Iteration: 186
Encoder Loss: 0.053 Iteration: 187
Encoder Loss: 0.054 Iteration: 188
Encoder Loss: 0.053 Iteration: 189
Encoder Loss: 0.053 Iteration: 190
Encoder Loss: 0.057 Iteration: 191
Encoder Loss: 0.062 Iteration: 192
Encoder Loss: 0.058 Iteration: 193
Encoder Loss: 0.057 Iteration: 194
Encoder Loss: 0.059 Iteration: 195
Encoder Loss: 0.056 Iteration: 196
Encoder Loss: 0.052 Iteration: 197
Encoder Loss: 0.052 Iteration: 198
Encoder Loss: 0.057 Iteration: 199
Encoder Loss: 0.055 Iteration: 200
Encoder Loss: 0.062 Iteration: 201
Encoder Loss: 0.058 Iteration: 202
Encoder Loss: 0.057 Iteration: 203
Encoder Loss: 0.057 Iteration: 204
Encoder Loss: 0.059 Iteration: 205
Encoder Loss: 0.053 Iteration: 206
Encoder Loss: 0.06 Iteration: 207
Encoder Loss: 0.059 Iteration: 208
Encoder Loss: 0.058 Iteration: 209
Encoder Loss: 0.059 Iteration: 210
Encoder Loss: 0.057 Iteration: 211
Encoder Loss: 0.056 Iteration: 212
Encoder Loss: 0.058 Iteration: 213
Encoder Loss: 0.054 Iteration: 214
Encoder Loss: 0.056 Iteration: 215
Encoder Loss: 0.062 Iteration: 216
Encoder Loss: 0.051 Iteration: 217
Encoder Loss: 0.063 Iteration: 218
Encoder Loss: 0.056 Iteration: 219
Encoder Loss: 0.061 Iteration: 220
Encoder Loss: 0.055 Iteration: 221
Encoder Loss: 0.061 Iteration: 222
Encoder Loss: 0.058 Iteration: 223
Encoder Loss: 0.057 Iteration: 224
Encoder Loss: 0.054 Iteration: 225
Encoder Loss: 0.058 Iteration: 226
Encoder Loss: 0.057 Iteration: 227
Encoder Loss: 0.062 Iteration: 228
Encoder Loss: 0.058 Iteration: 229
Encoder Loss: 0.062 Iteration: 230
Encoder Loss: 0.059 Iteration: 231
Encoder Loss: 0.058 Iteration: 232
Encoder Loss: 0.058 Iteration: 233
Encoder Loss: 0.057 Iteration: 234
Encoder Loss: 0.06 Iteration: 235
Encoder Loss: 0.05 Iteration: 236
Encoder Loss: 0.052 Iteration: 237
Encoder Loss: 0.051 Iteration: 238
Encoder Loss: 0.059 Iteration: 239
Encoder Loss: 0.058 Iteration: 240
Encoder Loss: 0.059 Iteration: 241
Encoder Loss: 0.059 Iteration: 242
Encoder Loss: 0.055 Iteration: 243
Encoder Loss: 0.054 Iteration: 244
Encoder Loss: 0.05 Iteration: 245
Encoder Loss: 0.06 Iteration: 246
Encoder Loss: 0.054 Iteration: 247
Encoder Loss: 0.058 Iteration: 248
Encoder Loss: 0.055 Iteration: 249
Encoder Loss: 0.059 Iteration: 250
Encoder Loss: 0.058 Iteration: 251
Encoder Loss: 0.058 Iteration: 252
Encoder Loss: 0.052 Iteration: 253
Encoder Loss: 0.054 Iteration: 254
Encoder Loss: 0.059 Iteration: 255
Encoder Loss: 0.051 Iteration: 256
Encoder Loss: 0.051 Iteration: 257
Encoder Loss: 0.056 Iteration: 258
Encoder Loss: 0.062 Iteration: 259
Encoder Loss: 0.056 Iteration: 260
Encoder Loss: 0.052 Iteration: 261
Encoder Loss: 0.058 Iteration: 262
Encoder Loss: 0.059 Iteration: 263
Encoder Loss: 0.052 Iteration: 264
Encoder Loss: 0.057 Iteration: 265
Encoder Loss: 0.06 Iteration: 266
Encoder Loss: 0.055 Iteration: 267
Encoder Loss: 0.055 Iteration: 268
Encoder Loss: 0.059 Iteration: 269
Encoder Loss: 0.053 Iteration: 270
Encoder Loss: 0.052 Iteration: 271
Encoder Loss: 0.056 Iteration: 272
Encoder Loss: 0.056 Iteration: 273
Encoder Loss: 0.057 Iteration: 274
Encoder Loss: 0.056 Iteration: 275
Encoder Loss: 0.057 Iteration: 276
Encoder Loss: 0.052 Iteration: 277
Encoder Loss: 0.05 Iteration: 278
Encoder Loss: 0.062 Iteration: 279
Encoder Loss: 0.057 Iteration: 280
Encoder Loss: 0.058 Iteration: 281
Encoder Loss: 0.053 Iteration: 282
Encoder Loss: 0.058 Iteration: 283
Encoder Loss: 0.058 Iteration: 284
Encoder Loss: 0.053 Iteration: 285
Encoder Loss: 0.06 Iteration: 286
Encoder Loss: 0.057 Iteration: 287
Encoder Loss: 0.053 Iteration: 288
Encoder Loss: 0.055 Iteration: 289
Encoder Loss: 0.061 Iteration: 290
Encoder Loss: 0.06 Iteration: 291
Encoder Loss: 0.056 Iteration: 292
Encoder Loss: 0.061 Iteration: 293
Encoder Loss: 0.057 Iteration: 294
Encoder Loss: 0.058 Iteration: 295
Encoder Loss: 0.058 Iteration: 296
Encoder Loss: 0.053 Iteration: 297
Encoder Loss: 0.06 Iteration: 298
Encoder Loss: 0.056 Iteration: 299
Encoder Loss: 0.056 Iteration: 300
Training Started... 
0: 0.1432996690273285
Training Epoch 0
train_metrics: 
R2: -12.90475082397461
MSE: 938512.5
MAPE: 0.48700201511383057
test metrics:
R2: -18.61092185974121
MSE: 1019340.6875
MAPE: 0.5681992769241333
before val features
MSE diff_p 0
----------
1: 0.05936656892299652
Training Epoch 1
train_metrics: 
R2: -111.18132019042969
MSE: 388809.5
MAPE: 0.4251684248447418
test metrics:
R2: -249.09530639648438
MSE: 527962.1875
MAPE: 0.5166938304901123
MSE diff_p 1
----------
2: 0.05074812099337578
Training Epoch 2
train_metrics: 
R2: -34.046363830566406
MSE: 332364.71875
MAPE: 0.3901064097881317
test metrics:
R2: -51.845401763916016
MSE: 439090.1875
MAPE: 0.49035775661468506
MSE diff_p 2
----------
3: 0.03163490071892738
Training Epoch 3
train_metrics: 
R2: -3.7883849143981934
MSE: 207186.46875
MAPE: 0.35331082344055176
test metrics:
R2: -4.673132419586182
MSE: 280036.375
MAPE: 0.42057004570961
MSE diff_p 3
----------
4: 0.028266197070479393
Training Epoch 4
train_metrics: 
R2: -1.6998093128204346
MSE: 185123.828125
MAPE: 0.3352035880088806
test metrics:
R2: -2.084730625152588
MSE: 227565.703125
MAPE: 0.39008408784866333
MSE diff_p 4
----------
5: 0.02216806821525097
Training Epoch 5
train_metrics: 
R2: 0.014111638069152832
MSE: 145185.34375
MAPE: 0.3548104465007782
test metrics:
R2: -0.008471369743347168
MSE: 163715.40625
MAPE: 0.3466702401638031
MSE diff_p 5
----------
6: 0.023148467764258385
Training Epoch 6
train_metrics: 
R2: -0.201460599899292
MSE: 151606.265625
MAPE: 0.2945820093154907
test metrics:
R2: -0.2820035219192505
MSE: 169299.171875
MAPE: 0.354403018951416
MSE diff_p 6
----------
7: 0.015606267377734184
Training Epoch 7
train_metrics: 
R2: 0.5148223638534546
MSE: 102210.140625
MAPE: 0.32537841796875
test metrics:
R2: 0.4342038035392761
MSE: 122992.203125
MAPE: 0.3226754665374756
MSE diff_p 7
----------
8: 0.012745944783091545
Training Epoch 8
train_metrics: 
R2: 0.5538473129272461
MSE: 83477.015625
MAPE: 0.2001315951347351
test metrics:
R2: 0.46008002758026123
MSE: 109483.25
MAPE: 0.27367740869522095
MSE diff_p 8
----------
9: 0.011939655989408493
Training Epoch 9
train_metrics: 
R2: 0.6663271188735962
MSE: 78196.390625
MAPE: 0.21993513405323029
test metrics:
R2: 0.6266982555389404
MSE: 91288.7734375
MAPE: 0.2541528642177582
MSE diff_p 9
----------
10: 0.010864353738725185
Training Epoch 10
train_metrics: 
R2: 0.6662319898605347
MSE: 71153.921875
MAPE: 0.18923252820968628
test metrics:
R2: 0.619213342666626
MSE: 89579.9453125
MAPE: 0.2440618872642517
MSE diff_p 10
----------
11: 0.00801951065659523
Training Epoch 11
train_metrics: 
R2: 0.7671120166778564
MSE: 52522.18359375
MAPE: 0.21257293224334717
test metrics:
R2: 0.6948548555374146
MSE: 80772.1484375
MAPE: 0.24930071830749512
MSE diff_p 11
----------
12: 0.009365906938910484
Training Epoch 12
train_metrics: 
R2: 0.7224899530410767
MSE: 61340.1484375
MAPE: 0.1723116934299469
test metrics:
R2: 0.6891946792602539
MSE: 80309.1328125
MAPE: 0.24072957038879395
MSE diff_p 12
----------
13: 0.009069561958312988
Training Epoch 13
train_metrics: 
R2: 0.7514302134513855
MSE: 59399.2890625
MAPE: 0.21268725395202637
test metrics:
R2: 0.7152475118637085
MSE: 77035.53125
MAPE: 0.24982726573944092
MSE diff_p 13
----------
14: 0.008369638584554195
Training Epoch 14
train_metrics: 
R2: 0.7810342907905579
MSE: 54815.27734375
MAPE: 0.20340998470783234
test metrics:
R2: 0.7017852067947388
MSE: 80579.25
MAPE: 0.25684913992881775
MSE diff_p 14
----------
15: 0.008650217205286026
Training Epoch 15
train_metrics: 
R2: 0.8011270761489868
MSE: 56652.875
MAPE: 0.24005845189094543
test metrics:
R2: 0.7625478506088257
MSE: 69618.71875
MAPE: 0.24690137803554535
MSE diff_p 15
----------
16: 0.007336275652050972
Training Epoch 16
train_metrics: 
R2: 0.82054603099823
MSE: 48047.4765625
MAPE: 0.15753015875816345
test metrics:
R2: 0.7823861241340637
MSE: 65627.3203125
MAPE: 0.22746612131595612
MSE diff_p 16
----------
17: 0.005387160927057266
Training Epoch 17
train_metrics: 
R2: 0.8650605082511902
MSE: 35282.13671875
MAPE: 0.16946521401405334
test metrics:
R2: 0.8088805079460144
MSE: 57855.5078125
MAPE: 0.21247044205665588
MSE diff_p 17
----------
18: 0.00587795116007328
Training Epoch 18
train_metrics: 
R2: 0.8627479672431946
MSE: 38496.46875
MAPE: 0.1587507724761963
test metrics:
R2: 0.8291784524917603
MSE: 54940.8671875
MAPE: 0.20332203805446625
MSE diff_p 18
----------
19: 0.006449750624597073
Training Epoch 19
train_metrics: 
R2: 0.8695790767669678
MSE: 42241.35546875
MAPE: 0.1698991060256958
test metrics:
R2: 0.8514957427978516
MSE: 48952.640625
MAPE: 0.1995113044977188
MSE diff_p 19
----------
20: 0.004709264729171991
Training Epoch 20
train_metrics: 
R2: 0.8945595622062683
MSE: 30842.388671875
MAPE: 0.14304083585739136
test metrics:
R2: 0.8605936765670776
MSE: 47937.859375
MAPE: 0.19287127256393433
MSE diff_p 20
----------
21: 0.005351276136934757
Training Epoch 21
train_metrics: 
R2: 0.8855634927749634
MSE: 35047.11328125
MAPE: 0.17628952860832214
test metrics:
R2: 0.8671213388442993
MSE: 44950.953125
MAPE: 0.1958802491426468
MSE diff_p 21
----------
22: 0.005552367307245731
Training Epoch 22
train_metrics: 
R2: 0.8865324258804321
MSE: 36364.125
MAPE: 0.16031873226165771
test metrics:
R2: 0.8776269555091858
MSE: 43795.3046875
MAPE: 0.18698638677597046
MSE diff_p 22
----------
23: 0.004785289987921715
Training Epoch 23
train_metrics: 
R2: 0.9002451300621033
MSE: 31340.302734375
MAPE: 0.1590283364057541
test metrics:
R2: 0.8815494775772095
MSE: 41316.60546875
MAPE: 0.19267156720161438
MSE diff_p 23
----------
24: 0.005165378097444773
Training Epoch 24
train_metrics: 
R2: 0.9021438360214233
MSE: 33829.61328125
MAPE: 0.16978436708450317
test metrics:
R2: 0.8786298632621765
MSE: 45077.4375
MAPE: 0.19707873463630676
MSE diff_p 24
----------
25: 0.004749132785946131
Training Epoch 25
train_metrics: 
R2: 0.8905168175697327
MSE: 31103.5
MAPE: 0.2008211463689804
test metrics:
R2: 0.8780326247215271
MSE: 43178.66015625
MAPE: 0.20939268171787262
MSE diff_p 25
----------
26: 0.004120585508644581
Training Epoch 26
train_metrics: 
R2: 0.9195814728736877
MSE: 26986.953125
MAPE: 0.13343530893325806
test metrics:
R2: 0.887948215007782
MSE: 41617.234375
MAPE: 0.193092942237854
MSE diff_p 26
----------
27: 0.004170499742031097
Training Epoch 27
train_metrics: 
R2: 0.9129707217216492
MSE: 27313.859375
MAPE: 0.1505807638168335
test metrics:
R2: 0.8912228345870972
MSE: 38456.19921875
MAPE: 0.1879318207502365
MSE diff_p 27
----------
28: 0.003411073936149478
Training Epoch 28
train_metrics: 
R2: 0.9262006282806396
MSE: 22340.150390625
MAPE: 0.12598441541194916
test metrics:
R2: 0.8973680138587952
MSE: 38927.6484375
MAPE: 0.18233314156532288
MSE diff_p 28
----------
29: 0.003534710733219981
Training Epoch 29
train_metrics: 
R2: 0.9278724193572998
MSE: 23149.8828125
MAPE: 0.13914334774017334
test metrics:
R2: 0.8993760347366333
MSE: 36292.203125
MAPE: 0.182918980717659
MSE diff_p 29
----------
30: 0.003117940854281187
Training Epoch 30
train_metrics: 
R2: 0.9375845193862915
MSE: 20420.3359375
MAPE: 0.12288079410791397
test metrics:
R2: 0.909943163394928
MSE: 34536.65625
MAPE: 0.17099900543689728
MSE diff_p 30
----------
31: 0.0028997936751693487
Training Epoch 31
train_metrics: 
R2: 0.9429659247398376
MSE: 18991.62109375
MAPE: 0.13670282065868378
test metrics:
R2: 0.9134793877601624
MSE: 32737.513671875
MAPE: 0.17017778754234314
MSE diff_p 31
----------
32: 0.002883385866880417
Training Epoch 32
train_metrics: 
R2: 0.9393625259399414
MSE: 18884.162109375
MAPE: 0.11193084716796875
test metrics:
R2: 0.9177749156951904
MSE: 31484.306640625
MAPE: 0.16369277238845825
MSE diff_p 32
----------
33: 0.0028864778578281403
Training Epoch 33
train_metrics: 
R2: 0.9437795877456665
MSE: 18904.4140625
MAPE: 0.14217624068260193
test metrics:
R2: 0.9180622100830078
MSE: 31193.787109375
MAPE: 0.16602951288223267
MSE diff_p 33
----------
34: 0.0029632379300892353
Training Epoch 34
train_metrics: 
R2: 0.9374907612800598
MSE: 19407.13671875
MAPE: 0.13036899268627167
test metrics:
R2: 0.9233812093734741
MSE: 30099.31640625
MAPE: 0.1602867841720581
MSE diff_p 34
----------
35: 0.0026587063912302256
Training Epoch 35
train_metrics: 
R2: 0.94920814037323
MSE: 17412.66796875
MAPE: 0.11719685792922974
test metrics:
R2: 0.9260724186897278
MSE: 28841.703125
MAPE: 0.15764589607715607
MSE diff_p 35
----------
36: 0.0029961413238197565
Training Epoch 36
train_metrics: 
R2: 0.939592182636261
MSE: 19622.630859375
MAPE: 0.11156230419874191
test metrics:
R2: 0.9274284839630127
MSE: 28696.6640625
MAPE: 0.15766571462154388
MSE diff_p 36
----------
37: 0.003234727308154106
Training Epoch 37
train_metrics: 
R2: 0.9381121397018433
MSE: 21185.20703125
MAPE: 0.14084291458129883
test metrics:
R2: 0.9253079891204834
MSE: 28779.017578125
MAPE: 0.16163498163223267
MSE diff_p 37
----------
38: 0.0034396194387227297
Training Epoch 38
train_metrics: 
R2: 0.9378107786178589
MSE: 22527.1015625
MAPE: 0.1351202428340912
test metrics:
R2: 0.9299426078796387
MSE: 28186.267578125
MAPE: 0.15648888051509857
MSE diff_p 38
----------
39: 0.002928185509517789
Training Epoch 39
train_metrics: 
R2: 0.9484940767288208
MSE: 19177.5703125
MAPE: 0.12467354536056519
test metrics:
R2: 0.9342316389083862
MSE: 26726.16015625
MAPE: 0.15534386038780212
MSE diff_p 39
----------
40: 0.00253370706923306
Training Epoch 40
train_metrics: 
R2: 0.9495409727096558
MSE: 16594.009765625
MAPE: 0.12405566871166229
test metrics:
R2: 0.9352661371231079
MSE: 26029.7578125
MAPE: 0.15382075309753418
MSE diff_p 40
----------
41: 0.002730177715420723
Training Epoch 41
train_metrics: 
R2: 0.9523218274116516
MSE: 17880.75390625
MAPE: 0.1283479928970337
test metrics:
R2: 0.9373003244400024
MSE: 25563.7421875
MAPE: 0.15146875381469727
MSE diff_p 41
----------
42: 0.0027774705085903406
Training Epoch 42
train_metrics: 
R2: 0.9467356204986572
MSE: 18190.48828125
MAPE: 0.11755140870809555
test metrics:
R2: 0.9363496899604797
MSE: 25359.705078125
MAPE: 0.15197589993476868
MSE diff_p 42
----------
43: 0.002531480509787798
Training Epoch 43
train_metrics: 
R2: 0.9466110467910767
MSE: 16579.42578125
MAPE: 0.10888022929430008
test metrics:
R2: 0.9363939762115479
MSE: 25294.970703125
MAPE: 0.1544705480337143
MSE diff_p 43
----------
44: 0.002480004681274295
Training Epoch 44
train_metrics: 
R2: 0.9525957703590393
MSE: 16242.2958984375
MAPE: 0.10263529419898987
test metrics:
R2: 0.9390758872032166
MSE: 24709.36328125
MAPE: 0.15037447214126587
MSE diff_p 44
----------
45: 0.002541509922593832
Training Epoch 45
train_metrics: 
R2: 0.9481990337371826
MSE: 16645.11328125
MAPE: 0.13485655188560486
test metrics:
R2: 0.939178466796875
MSE: 24507.328125
MAPE: 0.15329772233963013
MSE diff_p 45
----------
46: 0.003023429773747921
Training Epoch 46
train_metrics: 
R2: 0.9412692785263062
MSE: 19801.3515625
MAPE: 0.11644947528839111
test metrics:
R2: 0.9418767094612122
MSE: 24010.69921875
MAPE: 0.1483486294746399
MSE diff_p 46
----------
47: 0.0024710404686629772
Training Epoch 47
train_metrics: 
R2: 0.9555887579917908
MSE: 16183.587890625
MAPE: 0.12245076894760132
test metrics:
R2: 0.937705397605896
MSE: 25211.966796875
MAPE: 0.15994304418563843
MSE diff_p 47
----------
48: 0.0028271859046071768
Training Epoch 48
train_metrics: 
R2: 0.9420590400695801
MSE: 18516.08984375
MAPE: 0.1110023558139801
test metrics:
R2: 0.9402173161506653
MSE: 25312.015625
MAPE: 0.1532585620880127
MSE diff_p 48
----------
49: 0.003085519652813673
Training Epoch 49
train_metrics: 
R2: 0.938667893409729
MSE: 20207.99609375
MAPE: 0.13762471079826355
test metrics:
R2: 0.933953046798706
MSE: 26207.01171875
MAPE: 0.169432133436203
MSE diff_p 49
----------
50: 0.0036258765030652285
Training Epoch 50
train_metrics: 
R2: 0.9372207522392273
MSE: 23746.95703125
MAPE: 0.13695164024829865
test metrics:
R2: 0.9355667233467102
MSE: 28179.203125
MAPE: 0.1613880693912506
MSE diff_p 50
----------
51: 0.003690563142299652
Training Epoch 51
train_metrics: 
R2: 0.9322159886360168
MSE: 24170.607421875
MAPE: 0.1490485817193985
test metrics:
R2: 0.9345201253890991
MSE: 25887.07421875
MAPE: 0.16788217425346375
MSE diff_p 51
----------
52: 0.002682437188923359
Training Epoch 52
train_metrics: 
R2: 0.9508897662162781
MSE: 17568.0859375
MAPE: 0.11648916453123093
test metrics:
R2: 0.9403384327888489
MSE: 25896.8984375
MAPE: 0.15529704093933105
MSE diff_p 52
----------
53: 0.002930257935076952
Training Epoch 53
train_metrics: 
R2: 0.9342867136001587
MSE: 19191.142578125
MAPE: 0.12374528497457504
test metrics:
R2: 0.9346345663070679
MSE: 25332.037109375
MAPE: 0.16079236567020416
MSE diff_p 53
----------
54: 0.0034632650204002857
Training Epoch 54
train_metrics: 
R2: 0.9329310655593872
MSE: 22681.962890625
MAPE: 0.12235336005687714
test metrics:
R2: 0.9468432068824768
MSE: 22500.1875
MAPE: 0.14559219777584076
MSE diff_p 54
----------
55: 0.0027829918544739485
Training Epoch 55
train_metrics: 
R2: 0.9418632984161377
MSE: 18226.650390625
MAPE: 0.13511011004447937
test metrics:
R2: 0.9396369457244873
MSE: 24155.2109375
MAPE: 0.1611417829990387
MSE diff_p 55
----------
56: 0.003176099620759487
Training Epoch 56
train_metrics: 
R2: 0.9458103775978088
MSE: 20801.232421875
MAPE: 0.13051283359527588
test metrics:
R2: 0.9374616146087646
MSE: 26919.779296875
MAPE: 0.16410322487354279
MSE diff_p 56
----------
57: 0.0035073221661150455
Training Epoch 57
train_metrics: 
R2: 0.9302566647529602
MSE: 22970.5078125
MAPE: 0.14498500525951385
test metrics:
R2: 0.9364370107650757
MSE: 25012.30078125
MAPE: 0.16291233897209167
MSE diff_p 57
----------
58: 0.0036850066389888525
Training Epoch 58
train_metrics: 
R2: 0.9317355751991272
MSE: 24134.2109375
MAPE: 0.125480055809021
test metrics:
R2: 0.9391689896583557
MSE: 26416.37109375
MAPE: 0.16038793325424194
MSE diff_p 58
----------
59: 0.003915015608072281
Training Epoch 59
train_metrics: 
R2: 0.9246059656143188
MSE: 25640.6171875
MAPE: 0.15853457152843475
test metrics:
R2: 0.934972882270813
MSE: 25731.9453125
MAPE: 0.16930097341537476
MSE diff_p 59
----------
